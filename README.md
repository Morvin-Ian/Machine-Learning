# Machine Learning Learning Path

A comprehensive, structured curriculum for learning machine learning from fundamentals to advanced techniques.

## ğŸ“š Project Structure

This repository is organized into a clear learning progression with three main sections:

### [01-Fundamentals](./01-fundamentals)
Core mathematical concepts and algorithms essential for all machine learning work.

- **[Gradient Descent](./01-fundamentals/gradient-descent/notes.md)** - The optimization algorithm that powers modern machine learning
  - Intuitive explanation with visual analogies
  - Mathematical derivation and update rules
  - Variants: Batch, Stochastic, Mini-batch
  - Learning rate and convergence
  - Advanced optimizers: Momentum, Adam

### [02-Supervised Learning](./02-supervised-learning)
Algorithms that learn from labeled training data to make predictions.

- **[Linear Regression](./02-supervised-learning/linear-regression/notes.md)** - Predicting continuous values
- **[Logistic Regression](./02-supervised-learning/logistic-regression/notes.md)** - Binary and multi-class classification
- **[Classification](./02-supervised-learning/classification/notes.md)** - Evaluation metrics and thresholds

### [03-Unsupervised Learning](./03-unsupervised-learning)
Algorithms that discover patterns in unlabeled data.

- **[Clustering](./03-unsupervised-learning/clustering/notes.md)** - K-Means, Hierarchical, DBSCAN
- **[Dimensionality Reduction](./03-unsupervised-learning/dimensionality-reduction/notes.md)** - PCA, t-SNE
- **[Anomaly Detection](./03-unsupervised-learning/anomaly-detection/notes.md)** - Isolation Forest, One-Class SVM

## ğŸš€ Getting Started

### Prerequisites

- Python 3.12+
- pip or uv package manager

### Installation

```bash
# Clone the repository
git clone <repository-url>
cd machine-learning

# Install dependencies
pip install -r requirements.txt
# or with uv:
uv sync
```

### Recommended Learning Order

1. **Start here**: [01-Fundamentals](./01-fundamentals)
   - Build strong mathematical foundations
   - Master gradient descent optimization

2. **Then explore**: [02-Supervised Learning](./02-supervised-learning)
   - Begin with linear regression for regression basics
   - Progress to logistic regression for classification
   - Advance to classification metrics and evaluation

3. **Finally master**: [03-Unsupervised Learning](./03-unsupervised-learning)
   - Start with clustering (K-Means)
   - Learn dimensionality reduction (PCA)
   - Explore anomaly detection

## ğŸ“‹ Dependencies

See `pyproject.toml` for full list. Key packages include:

- **numpy** - Numerical computing
- **pandas** - Data manipulation
- **matplotlib & plotly** - Data visualization
- **scikit-learn** - Machine learning algorithms
- **tensorflow & keras** - Deep learning

## ğŸ“– How to Use This Repository

Each section contains:
- **notes.md** - Detailed explanations, theory, and examples
- **code files** - Practical implementations
- **README.md** - Section-specific guidance and prerequisites

Start with the notes to understand the theory, then explore the code implementations.

## ğŸ”— File Structure Overview

```
machine-learning/
â”œâ”€â”€ README.md                           (This file)
â”œâ”€â”€ pyproject.toml                      (Project configuration)
â”‚
â”œâ”€â”€ 01-fundamentals/
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ gradient-descent/
â”‚       â””â”€â”€ notes.md                    (Comprehensive GD guide)
â”‚
â”œâ”€â”€ 02-supervised-learning/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ linear-regression/
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ model.py
â”‚   â”‚   â””â”€â”€ notes.md
â”‚   â”œâ”€â”€ logistic-regression/
â”‚   â”‚   â””â”€â”€ notes.md
â”‚   â””â”€â”€ classification/
â”‚       â””â”€â”€ notes.md
â”‚
â””â”€â”€ 03-unsupervised-learning/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ notes.md                        (Overview)
    â”œâ”€â”€ clustering/
    â”‚   â””â”€â”€ notes.md                    (K-Means, Hierarchical, DBSCAN)
    â”œâ”€â”€ dimensionality-reduction/
    â”‚   â””â”€â”€ notes.md                    (PCA, t-SNE)
    â””â”€â”€ anomaly-detection/
        â””â”€â”€ notes.md                    (Isolation Forest, etc.)
```

## ğŸ’¡ Tips for Success

1. **Understand the theory** - Read the notes before running code
2. **Experiment** - Modify code examples and explore variations
3. **Practice** - Implement algorithms from scratch when possible
4. **Apply** - Find datasets and apply techniques to real problems
5. **Reference** - Bookmark official documentation for libraries used

## ğŸ¤ Contributing

Feel free to improve this learning path! Suggestions are welcome.

## ğŸ“ License

[Add your license information here]

---

**Happy Learning!** ğŸ“

Start with [01-fundamentals/gradient-descent](./01-fundamentals/gradient-descent/notes.md) and follow the learning path outlined above.
